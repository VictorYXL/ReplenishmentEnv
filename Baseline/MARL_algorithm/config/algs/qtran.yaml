# ======== Evaluation and Logging ========

test_greedy: True                       # Use greedy evaluation (if False, will set epsilon floor to 0)
test_nepisode: 10                       # Evaluate the policy at the end of training for test_nepsidoes episodes
test_interval: 50000                    # Evaluate the policy for every test_interval iterations
log_interval: 10000                     # Print stats to console for every log_interval iterations
visualize: False                        # Whether to visualize the policy
visualize_interval: 500000                # Visualize the policy for every visualize_interval iterations
# runner_log_interval: 10000            # Logging in the runner (deprecated)
learner_log_interval: 10000           # Logging in the learner (deprecated)
save_model: True                        # Whether save the models to disk
save_model_interval: 1460000            # Save the model for every save_model_interval iterations
checkpoint_path: ""                     # If not empty, load a mac checkpoint from this path
evaluate: False                         # Evaluate model for test_nepisode episodes and quit (no training)(deprecated)
save_replay: False                      # Saving the replay of the model loaded from checkpoint_path (deprecated)
local_results_path: "results"           # Path for local results
use_wandb: True                         # Whether log results to wandb
wandb_project_name: "whittle_index"     # The project name of the wandb project
use_tensorboard: False                  # Whether log results to tensorboard

# ======== Sampling ========

runner: "parallel"                      # The runner used to collect samples for training
batch_size: 8                           # Collect batch_size episodes for training in each iteration
batch_size_run: 2                       # Run batch_size_run parallel environments in the runner
buffer_size: 200                        # The maximum number of episodes stored in the buffer;
                                        # 5000 volume = 500 / 2 = 250 iterations
buffer_cpu_only: True                   # Whether to store the buffer in the CPU memory

# ======== Algorithm ========

name: "qtran"                          # The name of the algorithm
run: "qtran_run"                          # Which runner REGISTER to use
use_cuda: True                          # Whether to use GPU
seed: 101                               # The random seed used in numpy, torch, etc.
t_max: 5020000                          # Train t_max iterations in total

action_selector: "epsilon_greedy"       # How to select actions during the training (and maybe evaluation depending on test_greedy)
epsilon_start: 1.0                      # Annealing from epsilon_start to epsilon_finish linearly in epsilon_anneal_time iterations
epsilon_finish: 0.05
epsilon_anneal_time: 50000
save_probs: False                       # Whether to also return the action selection probablity from the action selector

learner: "qtran_learner"              # Which learner REGISTER to use
use_double_q: True                          # Whether to use double Q 
target_update_interval: 200             # Update the target Q network after target_update_interval iterations
mixer: "qtran_base"
qtran_arch: "qtran_paper"
network_size: "small"
mixing_embed_dim: 64
opt_loss: 1
nopt_min_loss: 0.1

mac: "dqn_mac"                          # Which multi-agent controller REGISTER to use
agent: "n_rnn"                          # Which NN structure REGISTER to use for the Q network and the Whittle index network;
                                        #   n_rnn indicates a shared-parameter RNN Q network for each agent (FC-GRU-FC)
use_layer_norm: False                   # Whether to use layer norm (deprecated)
use_orthogonal: False                   # Whether to use layer orthogonal (deprecated)
agent_output_type: "q"                  # What is the output format of the agent (Q network)
actor_input_seq_str: "o_la"             # The format of the inputs to the networks;
                                        #   o_la indicates the local state (observatoin) plus one-hot last actions (e.g., dim=86+34)
obs_last_action: True                   # Whether include the agent"s last action (one_hot) in the observation
obs_agent_id: False                     # Whether to append the one-hot agent id to the observation
hidden_dim: 128                         # The number of hidden units for the Q network
whittle_hidden_dim: 128                 # The number of hiddent units for the Whittle index network

accumulated_episodes: 8
gamma: 0.985                            # Discount rate for the MDP
optim: "Adam"                           # Whether to use "Adam" or "RMSprop"
lr: 0.0005                              # Learning rate used for Q network and Whittle index network training
optim_alpha: 0.99                       # alpha in RMSProp
optim_eps: 0.00001                      # epsilon in RMSProp
grad_norm_clip: 5                      # Reduce magnitude of gradients above this L2 norm
mini_epochs: 1
use_n_lambda: False                     # Whether to learn the Q values for different lambdas
n_lambda: 51                            # How many lambdas to use; 
                                        # We use lambdas = linspace(0, 10, 51) in the hard code, DO NOT CHANGE
use_individual_rewards: True            # Whether to train the agents using individual rewards
use_mean_team_reward: False             # Whether to train the agents using mean team rewards

use_reward_normalization: True          # Whether to normalize reward; 
                                        #   If True, we estimate a reward scaler for each lambda 
                                        #   and use it to transform the reward with these fixed scalers
use_loss_normalization: False           # Whether to normlize loss;
                                        #   If True, we divide the loss for each lambda with std
use_single_lambda_sampling: False       # Whether to collect samples with only one policy under the certain lambda;
                                        #   If False, we collect each episode using a randomly sampled lambda
sampling_lambda_index: 0                #   If True, collect sample use this lambda
use_sample_prob_weights: True           # Whether to multiply the loss by a weight
                                        #   If True, give weights equaling the probability of this sample collected by the 
                                        #   Q value under the lambda (assuming collecting using epsilon greedy with a large 
                                        #   epsilon) 


